---
layout: post
title: "An Intro to OLS Regression"
date: 2020-11-15
---

## OLS Regression: An ML Classic

_Prerequisite Math: Calculus (Derivatives), Introductory Statistics (Expectation, Normal Distribution)_

_Prerequisite Coding: Basic Python_


When we want to use a set of features X, to predict some response Y, one of the simplest (and oldest) approaches is to assume that
the predictor-response relationship is linear (ie can be modelled well with a straight line). This approach is intuitive, because
our brains deal with linear relationships every day. For example, many people are paid in wages (a linear relation between hours and pay), 
and most of us at least roughly obey the speed limit (a linear relation between distance and time) while driving. Another benefit of 
assuming a linear relationship is that the math becomes much more manageable, as we will see shortly.

\\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ \\]

### What is Ordinary Least Squares?

### How Do We Estimate $\beta$?

### Newton's Method

### Properties of the OLS Estimator

### Computational Concerns

### Application: Predicting Student Test Scores

### Other Extensions
- Weighted Least Squares
- Generalized Least Squares
- Non-linear Least Squares

### Further Reading

